"use strict";(self.webpackChunkvirtualclient=self.webpackChunkvirtualclient||[]).push([[8920],{29:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>a,frontMatter:()=>t,metadata:()=>d,toc:()=>h});var i=r(4848),s=r(8453);const t={},l="MLPerf",d={id:"workloads/mlperf/mlperf",title:"MLPerf",description:"MLPerf is a consortium of AI leaders from academia, research labs, and industry whose mission is to \u201cbuild fair and useful benchmarks\u201d that provide unbiased evaluations of training and inference performance for hardware, software, and services\u2014all conducted under prescribed conditions. To stay on the cutting edge of industry trends, MLPerf continues to evolve, holding new tests at regular intervals and adding new workloads that represent the state of the art in AI.",source:"@site/docs/workloads/mlperf/mlperf.md",sourceDirName:"workloads/mlperf",slug:"/workloads/mlperf/",permalink:"/VirtualClient/docs/workloads/mlperf/",draft:!1,unlisted:!1,editUrl:"https://github.com/microsoft/VirtualClient/edit/main/website/docs/workloads/mlperf/mlperf.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Memcached Workload Profiles",permalink:"/VirtualClient/docs/workloads/memcached/memcached-profiles"},next:{title:"MLPerf Developer Guide",permalink:"/VirtualClient/docs/workloads/mlperf/mlperf-developer-guide"}},c={},h=[{value:"System Requirements",id:"system-requirements",level:2},{value:"Supported Hardware Systems",id:"supported-hardware-systems",level:2},{value:"What is Being Measured?",id:"what-is-being-measured",level:2},{value:"Workload Metrics MLPerf Inference",id:"workload-metrics-mlperf-inference",level:2},{value:"Workload Metrics MLPerf Training",id:"workload-metrics-mlperf-training",level:2}];function o(e){const n={a:"a",br:"br",code:"code",del:"del",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"mlperf",children:"MLPerf"}),"\n",(0,i.jsx)(n.p,{children:"MLPerf is a consortium of AI leaders from academia, research labs, and industry whose mission is to \u201cbuild fair and useful benchmarks\u201d that provide unbiased evaluations of training and inference performance for hardware, software, and services\u2014all conducted under prescribed conditions. To stay on the cutting edge of industry trends, MLPerf continues to evolve, holding new tests at regular intervals and adding new workloads that represent the state of the art in AI."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/mlcommons/training_results_v2.1/blob/main/MLPerf%E2%84%A2%20Training%20v2.0%20Results%20Discussion.pdf",children:"MLPerf Training Documentation"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/mlcommons/inference_results_v4.1",children:"MLPerf Inference Documentation"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/mlcommons/training_results_v2.1/tree/main/NVIDIA/benchmarks",children:"MLPerf Training Benchmarks"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/mlcommons/inference_results_v4.1/tree/master/closed/NVIDIA",children:"MLPerf Inference Benchmarks"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/VirtualClient/docs/workloads/mlperf/mlperf-trainining-bert-preprocessing-data",children:"MLPerf Training Bert Preprocessing Data"})}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"system-requirements",children:"System Requirements"}),"\n",(0,i.jsx)(n.p,{children:"This is a GPU-specific workload and requires high-performance graphic cards to run. It is recommended that the system-under-test have a high-performing Nvidia (e.g. M60 or higher) or AMD (e.g. MI25 or higher)\ngraphics card."}),"\n",(0,i.jsx)(n.h2,{id:"supported-hardware-systems",children:"Supported Hardware Systems"}),"\n",(0,i.jsx)(n.p,{children:"The following section defines the hardware systems/SKUs on which the MLPerf workload will run effectively in cloud environments. These hardware systems contain\nGPU components for which the MLPerf workload is designed to test."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Datacenter systems MLPerf Inference"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A100-SXM4-40GBx8"}),"\n",(0,i.jsx)(n.li,{children:"A100-SXM-80GBx8 (NVIDIA DGX A100, 80GB variant)"}),"\n",(0,i.jsx)(n.li,{children:'A100-SXM-80GBx4 (NVIDIA DGX Station A100, "Red SEPTober", 80GB variant)'}),"\n",(0,i.jsx)(n.li,{children:"A100-PCIex8 (80GB variant)"}),"\n",(0,i.jsx)(n.li,{children:"A2x2"}),"\n",(0,i.jsx)(n.li,{children:"A30x8"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Edge Systems MLPerf Inference"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A100-SXM-80GBx1"}),"\n",(0,i.jsx)(n.li,{children:"A100-PCIex1 (80 GB variant)"}),"\n",(0,i.jsx)(n.li,{children:"A30x1"}),"\n",(0,i.jsx)(n.li,{children:"A2x1"}),"\n",(0,i.jsx)(n.li,{children:"Orin"}),"\n",(0,i.jsx)(n.li,{children:"Xavier NX"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Supported Config Files for MlPerf Bert Training (config_{nodes}x{gpus per node}x{local batch size}x{gradien accumulation}.sh)"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"config_A30_1x2x224x14.sh"}),"\n",(0,i.jsx)(n.li,{children:"config_DGXA100_1x4x56x2.sh"}),"\n",(0,i.jsx)(n.li,{children:"config_DGXA100_1x8x56x1.sh"}),"\n",(0,i.jsx)(n.li,{children:"config_DGXA100_4gpu_common.sh"}),"\n",(0,i.jsx)(n.li,{children:"config_DGXA100_512x8x2x1_pack.sh"}),"\n",(0,i.jsx)(n.li,{children:"config_DGXA100_8x8x48x1.sh"}),"\n",(0,i.jsx)(n.li,{children:"config_DGXA100_common.sh"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Source: ",(0,i.jsx)(n.a,{href:"https://github.com/mlcommons/training_results_v2.1/tree/main/NVIDIA/benchmarks/bert/implementations/pytorch-22.09",children:"link"})]}),"\n",(0,i.jsxs)(n.p,{children:["Additional details on whether a system is supported or not can be found in the documetation here,\nfor each benchmark check it's respective implementation folder :\n",(0,i.jsx)(n.a,{href:"https://github.com/mlcommons/training_results_v2.1/tree/main/NVIDIA/benchmarks",children:"https://github.com/mlcommons/training_results_v2.1/tree/main/NVIDIA/benchmarks"}),"\n",(0,i.jsx)(n.a,{href:"https://github.com/mlcommons/inference_results_v4.1/tree/master/closed/NVIDIA",children:"https://github.com/mlcommons/inference_results_v4.1/tree/master/closed/NVIDIA"})]}),"\n",(0,i.jsxs)(n.p,{children:["For systems which are not already included by MLPerf, add the config information in the appropriate __init__.py file under ",(0,i.jsx)(n.a,{href:"https://github.com/microsoft/VirtualClient/tree/main/src/VirtualClient/VirtualClient.Actions/MLPerf/GPUConfigFiles",children:"GPUConfigFiles"}),".",(0,i.jsx)(n.br,{}),"\n","For example with A100-SXM4-40GBx8, we have the following section in the 3d-unet, SingleStream file which is copied during initialization:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"@ConfigRegistry.register(HarnessType.Custom, AccuracyTarget.k_99, PowerSetting.MaxP)\nclass A100_SXM4_40GBx8(A100_SXM4_40GBx1):\n    system = KnownSystem.A100_SXM4_40GBx8\n    gpu_batch_size = {'3d-unet': 8}\n    start_from_device = True\n    end_on_device = True\n    single_stream_expected_latency_ns = 520000000\n"})}),"\n",(0,i.jsx)(n.h2,{id:"what-is-being-measured",children:"What is Being Measured?"}),"\n",(0,i.jsx)(n.p,{children:"GPU performance across a wide range of inference models. Work is planned for integrating support for training models as well."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Training Benchmarks"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"bert"}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.del,{children:"dlrm (not supported yet)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.del,{children:"maskrcnn (not supported yet)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.del,{children:"minigo (not supported yet)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.del,{children:"resnet (not supported yet)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.del,{children:"rnnt (not supported yet)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.del,{children:"ssd (not supported yet)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.del,{children:"unet3 (not supported yet)"})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Inference Benchmarks"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"bert"}),"\n",(0,i.jsx)(n.li,{children:"3d-unet"}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.del,{children:"dlrm-v2 (not supported yet)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.del,{children:"gptj (not supported yet)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.del,{children:"llama2-70b (not supported yet)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.del,{children:"mixtral-8x7b (not supported yet)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.del,{children:"resnet50 (not supported yet)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.del,{children:"retinanet (not supported yet)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.del,{children:"stable-diffusion-xl (not supported yet)"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"workload-metrics-mlperf-inference",children:"Workload Metrics MLPerf Inference"}),"\n",(0,i.jsx)(n.p,{children:"The following metrics are examples of those captured by the Virtual Client when running the MLPerf Inference workload."}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Scenario"}),(0,i.jsx)(n.th,{children:"Metric Name"}),(0,i.jsx)(n.th,{children:"Example Value"}),(0,i.jsx)(n.th,{children:"Unit"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"bert"}),(0,i.jsx)(n.td,{children:"PerformanceMode_p99"}),(0,i.jsx)(n.td,{children:"1.0"}),(0,i.jsx)(n.td,{children:"VALID/INVALID"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"bert"}),(0,i.jsx)(n.td,{children:"latency_ns_p99"}),(0,i.jsx)(n.td,{children:"525066834"}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"bert"}),(0,i.jsx)(n.td,{children:"samples_per_second_p99"}),(0,i.jsx)(n.td,{children:"25.2768"}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"bert"}),(0,i.jsx)(n.td,{children:"AccuracyMode_p99"}),(0,i.jsx)(n.td,{children:"1.0"}),(0,i.jsx)(n.td,{children:"PASS/FAIL"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"bert"}),(0,i.jsx)(n.td,{children:"AccuracyValue_p99"}),(0,i.jsx)(n.td,{children:"0.86112"}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"bert"}),(0,i.jsx)(n.td,{children:"ThresholdValue_p99"}),(0,i.jsx)(n.td,{children:"0.853083"}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"bert"}),(0,i.jsx)(n.td,{children:"AccuracyThresholdRatio_p99"}),(0,i.jsx)(n.td,{children:"1.00831923740128"}),(0,i.jsx)(n.td,{children:"PASS/FAIL"})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"workload-metrics-mlperf-training",children:"Workload Metrics MLPerf Training"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Scenario"}),(0,i.jsx)(n.th,{children:"Metric Name"}),(0,i.jsx)(n.th,{children:"Example Value (min)"}),(0,i.jsx)(n.th,{children:"Example Value (max)"}),(0,i.jsx)(n.th,{children:"Example Value (avg)"}),(0,i.jsx)(n.th,{children:"Unit"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"training-mlperf-bert-batchsize-45-gpu-8"}),(0,i.jsx)(n.td,{children:"eval_mlm_accuracy"}),(0,i.jsx)(n.td,{children:"0.650552854"}),(0,i.jsx)(n.td,{children:"0.672552854"}),(0,i.jsx)(n.td,{children:"0.662552854"}),(0,i.jsx)(n.td,{children:"%"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"training-mlperf-bert-batchsize-45-gpu-8"}),(0,i.jsx)(n.td,{children:"e2e_time"}),(0,i.jsx)(n.td,{children:"1071.040571"}),(0,i.jsx)(n.td,{children:"1078.040571"}),(0,i.jsx)(n.td,{children:"1074.040571"}),(0,i.jsx)(n.td,{children:"s"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"training-mlperf-bert-batchsize-45-gpu-8"}),(0,i.jsx)(n.td,{children:"training_sequences_per_second"}),(0,i.jsx)(n.td,{children:"2288.463615"}),(0,i.jsx)(n.td,{children:"2300.463615"}),(0,i.jsx)(n.td,{children:"2295.463615"}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"training-mlperf-bert-batchsize-45-gpu-8"}),(0,i.jsx)(n.td,{children:"final_loss"}),(0,i.jsx)(n.td,{children:"0"}),(0,i.jsx)(n.td,{children:"0"}),(0,i.jsx)(n.td,{children:"0"}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"training-mlperf-bert-batchsize-45-gpu-8"}),(0,i.jsx)(n.td,{children:"raw_train_time"}),(0,i.jsx)(n.td,{children:"1053.982237"}),(0,i.jsx)(n.td,{children:"1070.982237"}),(0,i.jsx)(n.td,{children:"1063.982237"}),(0,i.jsx)(n.td,{children:"s"})]})]})]})]})}function a(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(o,{...e})}):o(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>l,x:()=>d});var i=r(6540);const s={},t=i.createContext(s);function l(e){const n=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);