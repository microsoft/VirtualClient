"use strict";(self.webpackChunkvirtualclient=self.webpackChunkvirtualclient||[]).push([[5357],{2127:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>o,toc:()=>c});var i=r(4848),s=r(8453);const t={},a="MLPerf Developer Guide",o={id:"workloads/mlperf/mlperf-developer-guide",title:"MLPerf Developer Guide",description:"The following guide details the developer process for the MLPerf workload. The focus of this guide is on MLPerf Inference.",source:"@site/docs/workloads/mlperf/mlperf-developer-guide.md",sourceDirName:"workloads/mlperf",slug:"/workloads/mlperf/mlperf-developer-guide",permalink:"/VirtualClient/docs/workloads/mlperf/mlperf-developer-guide",draft:!1,unlisted:!1,editUrl:"https://github.com/microsoft/VirtualClient/edit/main/website/docs/workloads/mlperf/mlperf-developer-guide.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"MLPerf",permalink:"/VirtualClient/docs/workloads/mlperf/"},next:{title:"MLPerf Workload Profiles",permalink:"/VirtualClient/docs/workloads/mlperf/mlperf-profiles"}},l={},c=[{value:"Benchmarks",id:"benchmarks",level:2},{value:"Scenarios",id:"scenarios",level:2},{value:"Config Versions",id:"config-versions",level:2},{value:"Hardware for MLPerf",id:"hardware-for-mlperf",level:2},{value:"Adding a config",id:"adding-a-config",level:2},{value:"Dependencies",id:"dependencies",level:2},{value:"Running a Benchmark",id:"running-a-benchmark",level:2}];function d(e){const n={a:"a",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"mlperf-developer-guide",children:"MLPerf Developer Guide"}),"\n",(0,i.jsxs)(n.p,{children:["The following guide details the developer process for the MLPerf workload. The focus of this guide is on MLPerf ",(0,i.jsx)(n.em,{children:"Inference"}),"."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/VirtualClient/docs/workloads/mlperf/",children:"Workload Details"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/microsoft/VirtualClient/blob/main/src/VirtualClient/VirtualClient.Main/profiles/PERF-GPU-MLPERF.json",children:"Workload Profile"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/mlcommons/inference_results_v4.1",children:"MLPerf Inference Results Repository"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"In machine learning, inference involves using an already trained model to make predictions on unseen data.\nThe MLPerf workload will run multiple benchmarks on a GPU-based system, in which the performance of the model\nto make those predictions is measured. Throughput and latency are used as metrics."}),"\n",(0,i.jsx)(n.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,i.jsxs)(n.p,{children:["The benchmark suite for NVIDIA GPU-based systems in MLPerf Inference is detailed in the ",(0,i.jsx)(n.a,{href:"https://github.com/mlcommons/inference_results_v4.1/tree/main/closed/NVIDIA/configs",children:"Inference results\nrepository"}),".",(0,i.jsx)(n.br,{}),"\n","The following are supported currently in Virtual Client:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"bert"}),": Used for natural language processing tasks. This benchmark does not require any supplemental data to test."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"3d-unet"}),": Used for 3D volumetric data for medical imaging applications. This benchmark does not require any supplemental data to test."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"scenarios",children:"Scenarios"}),"\n",(0,i.jsxs)(n.p,{children:["MLPerf will evaluate the performance of a system in different scenarios. For a given benchmark, the configurations for each scenario\nare available under the ",(0,i.jsx)(n.a,{href:"https://github.com/mlcommons/inference_results_v4.1/tree/main/closed/NVIDIA/configs/bert",children:"directory for the benchmark"}),"."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Offline"}),": All queries are aggregated into a batch and sent to the tested system. The maximum throughput without a latency constraint is measured."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Server"}),": Queries are aggregated into multiple batches and sent to the tested system. The maximum throughput with a latency constraint is measured."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"SingleStream"}),": Queries are sent one-by-one to the tested system. The latency of processing individual queries is measured."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"config-versions",children:"Config Versions"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"default"}),": Uses lower precision to achieve faster inference times."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"high accuracy"}),": Uses higher precision and prioritizes accuracy over performance. (not supported yet)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"triton"}),": Uses the triton inference server to manage and serve models. (not supported yet)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"triton high accuracy"}),": Uses the triton inference server and higher precision. (not supported yet)"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"hardware-for-mlperf",children:"Hardware for MLPerf"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"A100_SXM4_40GBx8"}),": Azure VM SKU Standard_ND96asr_v4. This represents a system with 8 A100 NVIDIA GPUs. The NVIDIA A100 GPU\nis designed for high-performance computing."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"adding-a-config",children:"Adding a config"}),"\n",(0,i.jsxs)(n.p,{children:["The config information given the benchmark, scenario, config version, and system to test is in the ",(0,i.jsx)(n.a,{href:"https://github.com/mlcommons/inference_results_v4.1/blob/main/closed/NVIDIA/configs/bert/SingleStream/__init__.py",children:"__init__.py file under the benchmark folder"}),".\nBy default, the repository does not have support for all systems (i.e. A100_SXM4_40GBx8). To add support, the file is replaced with Virtual Client at runtime.\nFor example with bert in the SingleStream scenario, a file with the following section is used:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"@ConfigRegistry.register(HarnessType.Custom, AccuracyTarget.k_99, PowerSetting.MaxP)\nclass A100_SXM4_40GBx8(SingleStreamGPUBaseConfig):\n    system = KnownSystem.A100_SXM4_40GBx8\n    single_stream_expected_latency_ns = 1700000\n"})}),"\n",(0,i.jsxs)(n.p,{children:["These files are stored as script files for MLPerf, under ",(0,i.jsx)(n.a,{href:"https://github.com/microsoft/VirtualClient/tree/main/src/VirtualClient/VirtualClient.Actions/MLPerf/GPUConfigFiles",children:"GPUConfigFiles"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"dependencies",children:"Dependencies"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"make"}),", ",(0,i.jsx)(n.strong,{children:"gcc"}),": Necessary dependencies for the workload. Installed in CUDAAndNvidiaGPUDriverInstallation:"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'commands.Add("apt install build-essential -yq");\n'})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CUDA"}),": An API created by NVIDIA which enables general-purpose computing on GPUs. To install CUDA, a .run file is used."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"NVIDIA Linux Driver"}),": Software component that enables communication between GPUs and the operating\nsystem. The linux driver will handle the low-level interaction between the GPU and OS. This driver is\nrequired for CUDA to function.",(0,i.jsx)(n.br,{}),"\n","The values for CUDA and the Linux Driver are called out in profile parameters section."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'"Parameters": {\n    ...\n    "LinuxCudaVersion": "12.4",\n    "LinuxDriverVersion": "550",\n    "LinuxLocalRunFile": "https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda_12.4.0_550.54.14_linux.run",\n    ...\n}\n'})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"NVIDIA Fabric Manager"}),": A software stack which is used to connect multiple GPUs for high-performance computing tasks.",(0,i.jsx)(n.br,{}),"\n","CUDA, the NVIDIA linux driver, and the NVIDIA fabric manager are all installed using NvidiaCudaInstallation."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{\n    "Type": "NvidiaCudaInstallation",\n    "Parameters": {\n        "Scenario": "InstallNvidiaCuda",\n        "LinuxCudaVersion": "$.Parameters.LinuxCudaVersion",\n        "LinuxDriverVersion": "$.Parameters.LinuxDriverVersion",\n        "Username": "$.Parameters.Username",\n        "LinuxLocalRunFile": "$.Parameters.LinuxLocalRunFile"\n    }\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:["The versions of the linux driver and the fabric manager ",(0,i.jsx)(n.strong,{children:"must match exactly otherwise the fabric manager will not start and the benchmark\ncannot be run"}),"."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'commands.Add($"apt install nvidia-driver-{this.LinuxDriverVersion}-server nvidia-dkms-{this.LinuxDriverVersion}-server -y");\ncommands.Add($"apt install cuda-drivers-fabricmanager-{this.LinuxDriverVersion} -y");\n'})}),"\n",(0,i.jsx)(n.p,{children:"The values can be checked in the terminal:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"azureuser@mlperf-vm-1:~$ nv-fabricmanager --version\nFabric Manager version is : 550.127.05\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"azureuser@mlperf-vm-1:~$ nvidia-smi\nFri Nov 15 03:56:48 2024\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A100-SXM4-40GB          Off |   00000001:00:00.0 Off |                    0 |\n| N/A   31C    P0             53W /  400W |       1MiB /  40960MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA A100-SXM4-40GB          Off |   00000002:00:00.0 Off |                    0 |\n| N/A   30C    P0             52W /  400W |       1MiB /  40960MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   2  NVIDIA A100-SXM4-40GB          Off |   00000003:00:00.0 Off |                    0 |\n| N/A   30C    P0             51W /  400W |       1MiB /  40960MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   3  NVIDIA A100-SXM4-40GB          Off |   00000004:00:00.0 Off |                    0 |\n| N/A   31C    P0             55W /  400W |       1MiB /  40960MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   4  NVIDIA A100-SXM4-40GB          Off |   0000000B:00:00.0 Off |                    0 |\n| N/A   31C    P0             57W /  400W |       1MiB /  40960MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   5  NVIDIA A100-SXM4-40GB          Off |   0000000C:00:00.0 Off |                    0 |\n| N/A   30C    P0             52W /  400W |       1MiB /  40960MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   6  NVIDIA A100-SXM4-40GB          Off |   0000000D:00:00.0 Off |                    0 |\n| N/A   31C    P0             52W /  400W |       1MiB /  40960MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   7  NVIDIA A100-SXM4-40GB          Off |   0000000E:00:00.0 Off |                    0 |\n| N/A   30C    P0             53W /  400W |       1MiB /  40960MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Docker"}),": A platform to use containers. MLPerf inference will run benchmarks within a docker container.",(0,i.jsx)(n.br,{}),"\n","Docker is installed with DockerInstallation."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{\n    "Type": "DockerInstallation",\n    "Parameters": {\n        "Scenario": "InstallDocker"\n    }\n}\n'})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"NVIDIA Container Toolkit"}),": A set of tools which enable the use of NVIDIA GPUs within docker containers.",(0,i.jsx)(n.br,{}),"\n","Nvidia container toolkit is installed with NvidiaContainerToolkitInstallation."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{\n    "Type": "NvidiaContainerToolkitInstallation",\n    "Parameters": {\n        "Scenario": "InstallNvidiaContainerToolkit"\n    }\n}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"running-a-benchmark",children:"Running a Benchmark"}),"\n",(0,i.jsx)(n.p,{children:"There are a few setup steps before running the benchmark:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"make prebuild"}),": Download the docker container image and launch the container.\nThe remaining commands are run within the container. In order to avoid launching the docker container shell,\nthe file is replaced with Virtual Client. The ",(0,i.jsx)(n.a,{href:"https://github.com/microsoft/VirtualClient/blob/main/src/VirtualClient/VirtualClient.Actions/MLPerf/Makefile.docker",children:"replacing Makefile.docker file"}),"\ndoes not launch the docker container shell."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:'make download_data BENCHMARKS="bert"'}),": Download datasets necessary to run the benchmark."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:'make download_model BENCHMARKS="bert"'}),": Download pre-trained model to be tested."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:'make preprocess_data BENCHMARKS="bert"'}),": Formats data to be used in the benchmark."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"make build"}),": Compile and build the executable to run the benchmark."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"To actually run the benchmark:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"make run RUN_ARGS='--benchmarks=bert --scenarios=Offline,Server,SingleStream --config_ver=default --test_mode=PerformanceOnly --fast"}),": Run performance mode which focuses\non the efficiency of the model in making predictions. In this example, the command will run the bert benchmark, with Offline, Server, and Single Stream scenarios, using the default config version,\nin performance only mode, and with fewer iterations for faster turnaround time.",(0,i.jsx)(n.br,{}),"\n","The json output will include a valid/invalid output, and either the latency or throughput. For example this is the json output for the Single Stream scenario:"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{\n    "benchmark_full": "bert-99",\n    "benchmark_short": "bert",\n    "config_name": "DGX-A100_A100-SXM4-40GBx8_TRT-custom_k_99_MaxP-SingleStream",\n    "detected_system": "SystemConfiguration(host_cpu_conf=CPUConfiguration(layout={CPU(name=\\"AMD EPYC 7V12 64-Core Processor\\", architecture=CPUArchitecture.x86_64, core_count=48, threads_per_core=1): 2}), host_mem_conf=MemoryConfiguration(host_memory_capacity=Memory(quantity=928.7656999999999, byte_suffix=ByteSuffix.GB), comparison_tolerance=0.05), accelerator_conf=AcceleratorConfiguration(layout={GPU(name=\\"NVIDIA A100-SXM4-40GB\\", accelerator_type=AcceleratorType.Discrete, vram=Memory(quantity=40.0, byte_suffix=ByteSuffix.GiB), max_power_limit=400.0, pci_id=\\"0x20B010DE\\", compute_sm=80): 8}), numa_conf=NUMAConfiguration(numa_nodes={}, num_numa_nodes=4), system_id=\\"DGX-A100_A100-SXM4-40GBx8\\")",\n    "early_stopping_met": true,\n    "effective_min_duration_ms": 600000,\n    "effective_min_query_count": 100,\n    "result_90.00_percentile_latency_ns": 1924537,\n    "result_validity": "INVALID",\n    "satisfies_query_constraint": false,\n    "scenario": "SingleStream",\n    "scenario_key": "result_90.00_percentile_latency_ns",\n    "summary_string": "result_90.00_percentile_latency_ns: 1924537, Result is INVALID, 10-min runtime requirement met: True",\n    "system_name": "DGX-A100_A100-SXM4-40GBx8_TRT",\n    "tensorrt_version": "10.2.0",\n    "test_mode": "PerformanceOnly"\n}\n'})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"make run RUN_ARGS='--benchmarks=bert --scenarios=Offline,Server,SingleStream --config_ver=default --test_mode=AccuracyOnly --fast"}),": Run accuracy mode which focuses on\nthe accuracy of the model's predictions. In this example, the command will run the bert benchmark, with Offline, Server, and Single Stream scenarios, using the default config version,\nin accuracy only mode, and with fewer iterations for faster turnaround time.",(0,i.jsx)(n.br,{}),"\n","The json output will inculde a pass/fail output, and the accuracy score. For example this is the json output for the Offline scenario:"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{\n    "accuracy": [\n        {\n            "name": "F1",\n            "pass": true,\n            "threshold": 89.96526,\n            "value": 90.2147015680108\n        }\n    ],\n    "accuracy_pass": true,\n    "benchmark_full": "bert-99",\n    "benchmark_short": "bert",\n    "config_name": "DGX-A100_A100-SXM4-40GBx8_TRT-custom_k_99_MaxP-Offline",\n    "detected_system": "SystemConfiguration(host_cpu_conf=CPUConfiguration(layout={CPU(name=\\"AMD EPYC 7V12 64-Core Processor\\", architecture=CPUArchitecture.x86_64, core_count=48, threads_per_core=1): 2}), host_mem_conf=MemoryConfiguration(host_memory_capacity=Memory(quantity=928.7656999999999, byte_suffix=ByteSuffix.GB), comparison_tolerance=0.05), accelerator_conf=AcceleratorConfiguration(layout={GPU(name=\\"NVIDIA A100-SXM4-40GB\\", accelerator_type=AcceleratorType.Discrete, vram=Memory(quantity=40.0, byte_suffix=ByteSuffix.GiB), max_power_limit=400.0, pci_id=\\"0x20B010DE\\", compute_sm=80): 8}), numa_conf=NUMAConfiguration(numa_nodes={}, num_numa_nodes=4), system_id=\\"DGX-A100_A100-SXM4-40GBx8\\")",\n    "effective_min_duration_ms": 600000,\n    "effective_samples_per_query": 19800000,\n    "satisfies_query_constraint": true,\n    "scenario": "Offline",\n    "scenario_key": "result_samples_per_second",\n    "summary_string": "[PASSED] F1: 90.215 (Threshold=89.965)",\n    "system_name": "DGX-A100_A100-SXM4-40GBx8_TRT",\n    "tensorrt_version": "10.2.0",\n    "test_mode": "AccuracyOnly"\n}\n'})})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var i=r(6540);const s={},t=i.createContext(s);function a(e){const n=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);